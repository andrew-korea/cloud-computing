{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e637e494",
   "metadata": {},
   "source": [
    "# MongoDB PySpark Connector\n",
    "Official guide: https://docs.mongodb.com/spark-connector/current/python-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b25bf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.79.199.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>YouTube Trending Videos Analysis and Prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11eff56a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# stop the SparkSession created automatically (by the time the notebook is running, cannot change much in that session's configuration)\n",
    "# [Ref]https://www.edureka.co/community/5268/how-to-change-the-spark-session-configuration-in-pyspark\n",
    "spark.sparkContext.stop() \n",
    "# create a new SparkSession and connect to MongoDB database & collection\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"YouTube Trending Videos Analysis and Prediction\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://gp15:MSBD5003gp15@cluster0.3ygtx.mongodb.net/Database0.US\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://gp15:MSBD5003gp15@cluster0.qfnff.mongodb.net/Database0.US_preprocessed\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark # check if sparksession created successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88c3e5",
   "metadata": {},
   "source": [
    "## Read from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f1c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e355ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- categoryId: integer (nullable = true)\n",
      " |-- channelId: string (nullable = true)\n",
      " |-- channelTitle: string (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- publishedAt: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- view_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a3e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|summary|        view_count|            likes|          dislikes|     comment_count|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             88791|            88791|             88791|             88791|\n",
      "|   mean|2831642.3201563223|153998.5078217387| 3193.618305909383|14369.411674606661|\n",
      "| stddev| 7354353.791269992|436970.2866347495|13595.938144588574|100702.89357600761|\n",
      "|    min|                 0|                0|                 0|                 0|\n",
      "|    max|         264407389|         16021534|            879354|           6738537|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('view_count','likes','dislikes','comment_count').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff91358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of videos (duplicate count for same video on different date) 88791\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of videos (duplicate count for same video on different date)\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e24e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check number of rows with null entry the 10 columns of interest\n",
    "# 查询某列为null的行数\n",
    "from pyspark.sql.functions import isnull\n",
    "\n",
    "cols = ['trending_date','title','channelTitle',\n",
    "        'categoryId', 'publishedAt','tags','view_count',\n",
    "        'likes','dislikes','comment_count']\n",
    "for col in cols:\n",
    "    print(df.filter(isnull('%s'%col)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135feb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88791"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(thresh=10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b99ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique videos: 15422\n"
     ]
    }
   ],
   "source": [
    "# For considering unique videos only\n",
    "# remove duplicates for these columns\n",
    "df_unique = df.dropDuplicates(['video_id'])\n",
    "print(\"Total number of unique videos:\", df_unique.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c8903",
   "metadata": {},
   "source": [
    "### Read Category_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6aa1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = spark.read.format(\"mongo\").option(\"uri\", \"mongodb+srv://gp15:MSBD5003gp15@cluster0.3ygtx.mongodb.net/Database0.US_cat\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5a802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+\n",
      "|categoryTitle        |categoryId|\n",
      "+---------------------+----------+\n",
      "|Film & Animation     |1         |\n",
      "|Autos & Vehicles     |2         |\n",
      "|Music                |10        |\n",
      "|Pets & Animals       |15        |\n",
      "|Sports               |17        |\n",
      "|Short Movies         |18        |\n",
      "|Travel & Events      |19        |\n",
      "|Gaming               |20        |\n",
      "|Videoblogging        |21        |\n",
      "|People & Blogs       |22        |\n",
      "|Comedy               |23        |\n",
      "|Entertainment        |24        |\n",
      "|News & Politics      |25        |\n",
      "|Howto & Style        |26        |\n",
      "|Education            |27        |\n",
      "|Science & Technology |28        |\n",
      "|Nonprofits & Activism|29        |\n",
      "|Movies               |30        |\n",
      "|Anime/Animation      |31        |\n",
      "|Action/Adventure     |32        |\n",
      "+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Explode Array to Structure\n",
    "explodej = df_cat.withColumn('Exp_RESULTS',F.explode(F.col('items'))).drop('items')\n",
    "dfj = explodej.select(\"Exp_RESULTS.snippet.title\",'Exp_RESULTS.id')\n",
    "dfj=dfj.withColumnRenamed('id','categoryId')\n",
    "dfj=dfj.withColumnRenamed('title','categoryTitle')     # 将title重命名为category_title\n",
    "dfj.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6086cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df.join(dfj, 'categoryId', \"left_outer\")\n",
    "# df_.show()\n",
    "df_final = df_joined.select(\"trending_date\",\"video_id\",\"title\",\"categoryTitle\",\"channelTitle\",\"channelId\",\"tags\",\"description\",\"publishedAt\",\"view_count\",\"comment_count\",\"dislikes\",\"likes\",\"ratings_disabled\",\"comments_disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7282e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_final.toPandas()\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbd349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ebcddd",
   "metadata": {},
   "source": [
    "## (testing) Write to MongoDB\n",
    "### Notes:\n",
    "DataFrameWriter.mode(saveMode)\n",
    "\n",
    "-Specifies the behavior when data or table already exists. Options include:\n",
    "\n",
    "- append: Append contents of this DataFrame to existing data.\n",
    "\n",
    "- overwrite: Overwrite existing data.\n",
    "\n",
    "- error or errorifexists: Throw an exception if data already exists.\n",
    "\n",
    "- ignore: Silently ignore this operation if data already exists.\n",
    "\n",
    "[Ref] http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.mode.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# people = spark.createDataFrame([(\"Bilbo Baggins\",  60), (\"Gandalf\", 1000), (\"Thorin\", 195), (\"Balin\", 178), (\"Kili\", 77),\n",
    "#    (\"Dwalin\", 169), (\"Oin\", 167), (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", None)], [\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e38b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.format(\"mongo\").mode(\"overwrite\").option(\"database\", \"Database0\").option(\"collection\", \"US_preprocessed\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664014a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
